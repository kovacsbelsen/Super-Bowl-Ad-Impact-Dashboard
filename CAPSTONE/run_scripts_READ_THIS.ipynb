{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb1353f8",
   "metadata": {},
   "source": [
    "First the superbowl data has to be scraped from https://www.superbowl-ads.com to get a historical representation of past ads, run the scrape_superbowl_ads.py custom scraper to get the initial list, then run the extract_youtube_link.py to get youtube links if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71009b73",
   "metadata": {},
   "source": [
    "DASHBOARD LINK:\n",
    "https://app.snowflake.com/lywbbpj/odb66944/#/streamlit-apps/DATAEXPERT_STUDENT.BENCEKOVACS.EL98HTPA_K2BNB0U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4c57e",
   "metadata": {},
   "source": [
    "GITHUB LINK: https://github.com/kovacsbelsen/Super-Bowl-Ad-Impact-Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5df0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Python scripts from the ingest folder to scrape initial superbowl data (might need to change path, depending on how it's extracted on individual machines)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# ✅ Define the path to your scripts\n",
    "ingest_path = \"C:\\Work_Git\\DataEngineer\\CAPSTONE\\ingest\"\n",
    "\n",
    "# ✅ Define the list of scripts to run in order\n",
    "scripts_to_run = [\n",
    "    \"scrape_superbowl_ads.py\",\n",
    "    \"extract_youtube_link.py\",\n",
    "    \"merge_csv_data.py\",\n",
    "    \"summarize_superbowl_ads.py\",\n",
    "    \"get_all_youtube_update_into_dataset.py\"\n",
    "]\n",
    "\n",
    "# ✅ Execute each script\n",
    "for script in scripts_to_run:\n",
    "    script_path = os.path.join(ingest_path, script)\n",
    "    \n",
    "    if not os.path.isfile(script_path):\n",
    "        print(f\"❌ Script not found: {script}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"▶️ Running: {script}\")\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", script_path], check=True, capture_output=True, text=True)\n",
    "        print(f\"✅ Success: {script}\\n{result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Failed: {script}\\n{e.stderr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154242d9",
   "metadata": {},
   "source": [
    "Next step is slightly manual, use CHATGPT and manual research to find the associated Company, Brand and stock ticker values for each ad in order to enrich the data ( I did it for you)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42962f7c",
   "metadata": {},
   "source": [
    "Fetch stock and company data from polygon.io for the past 10 years or available stock tickers and save as CSV for EDA and data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Python scripts from the ingest folder to scrape initial superbowl data (might need to change path, depending on how it's extracted on individual machines)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# ✅ Define the path to your scripts\n",
    "ingest_path = \"C:\\Work_Git\\DataEngineer\\CAPSTONE\\ingest\"\n",
    "\n",
    "# ✅ Define the list of scripts to run in order\n",
    "scripts_to_run = [\n",
    "    \"poly_fetch_stock_data.py\",\n",
    "    \"poly_fetch_ticker_events.py\",\n",
    "    \"poly_fetch_ticker_financials.py\",\n",
    "    \"poly_fetch_ticker_fundamentals_company_metadata.py\",\n",
    "    \"poly_fetch_ticker_metadata.py\"\n",
    "]\n",
    "\n",
    "# ✅ Execute each script\n",
    "for script in scripts_to_run:\n",
    "    script_path = os.path.join(ingest_path, script)\n",
    "    \n",
    "    if not os.path.isfile(script_path):\n",
    "        print(f\"❌ Script not found: {script}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"▶️ Running: {script}\")\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", script_path], check=True, capture_output=True, text=True)\n",
    "        print(f\"✅ Success: {script}\\n{result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Failed: {script}\\n{e.stderr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276e95e",
   "metadata": {},
   "source": [
    "Important realisation during EDA, polygon.io has been missing a lot of data from stocks which are present in the superbowl ads, so a secondary data source was necessary.\n",
    "Some tickers were missing entirely, some did not have stock data, some did not have company metadata.\n",
    "\n",
    "Try to force update missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Python scripts from the ingest folder to scrape initial superbowl data (might need to change path, depending on how it's extracted on individual machines)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# ✅ Define the path to your scripts\n",
    "ingest_path = \"C:\\Work_Git\\DataEngineer\\CAPSTONE\\ingest\\update_missing_polygon_data\"\n",
    "\n",
    "# ✅ Define the list of scripts to run in order\n",
    "scripts_to_run = [\n",
    "    \"update_missing_company_data.py\",\n",
    "    \"update_missing_financials.py\",\n",
    "    \"update_missing_stock_data.py\",\n",
    "    \"update_missing_ticker_metadata.py\"\n",
    "]\n",
    "\n",
    "# ✅ Execute each script\n",
    "for script in scripts_to_run:\n",
    "    script_path = os.path.join(ingest_path, script)\n",
    "    \n",
    "    if not os.path.isfile(script_path):\n",
    "        print(f\"❌ Script not found: {script}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"▶️ Running: {script}\")\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", script_path], check=True, capture_output=True, text=True)\n",
    "        print(f\"✅ Success: {script}\\n{result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Failed: {script}\\n{e.stderr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68bea4f",
   "metadata": {},
   "source": [
    "Check data for missing info before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Python scripts from the ingest folder to scrape initial superbowl data (might need to change path, depending on how it's extracted on individual machines)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# ✅ Define the path to your scripts\n",
    "ingest_path = \"C:\\Work_Git\\DataEngineer\\CAPSTONE\\ingest\"\n",
    "\n",
    "# ✅ Define the list of scripts to run in order\n",
    "scripts_to_run = [\n",
    "    \"check_missing_tickers.py\",\n",
    "    \"count_matching_superbowl_stock_data.py\"\n",
    "]\n",
    "\n",
    "# ✅ Execute each script\n",
    "for script in scripts_to_run:\n",
    "    script_path = os.path.join(ingest_path, script)\n",
    "    \n",
    "    if not os.path.isfile(script_path):\n",
    "        print(f\"❌ Script not found: {script}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"▶️ Running: {script}\")\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", script_path], check=True, capture_output=True, text=True)\n",
    "        print(f\"✅ Success: {script}\\n{result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Failed: {script}\\n{e.stderr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d6778",
   "metadata": {},
   "source": [
    "Introduce yfinance.\n",
    "YahooFinance wrapper python library freely accessible, pull data for the stocks we are interested in from the superbowls ads and pull related stock data which are similar companies.\n",
    "\n",
    "To overview some of its data, check \"yahoo_read_stock_profile_data.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Python scripts from the ingest folder to scrape initial superbowl data (might need to change path, depending on how it's extracted on individual machines)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# ✅ Define the path to your scripts\n",
    "ingest_path = \"C:\\Work_Git\\DataEngineer\\CAPSTONE\\ingest\"\n",
    "\n",
    "# ✅ Define the list of scripts to run in order\n",
    "scripts_to_run = [\n",
    "    \"yahoo_fetch_stock_data.py\",\n",
    "    \"yahoo_fetch_financial_metadataa.py\",\n",
    "    \"yahoo_fetch_related_stickers.py\",\n",
    "    \"yahoo_fetch_related_stickers_stock_data_and_financial_data.py\",\n",
    "]\n",
    "\n",
    "# ✅ Execute each script\n",
    "for script in scripts_to_run:\n",
    "    script_path = os.path.join(ingest_path, script)\n",
    "    \n",
    "    if not os.path.isfile(script_path):\n",
    "        print(f\"❌ Script not found: {script}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"▶️ Running: {script}\")\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", script_path], check=True, capture_output=True, text=True)\n",
    "        print(f\"✅ Success: {script}\\n{result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Failed: {script}\\n{e.stderr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706de58",
   "metadata": {},
   "source": [
    "Manual checks for missing data by running \"check_missing_tickers.py\" for different csv files / folders, overall 2 missing tickers, good enough to proceed to Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63960639",
   "metadata": {},
   "source": [
    "Merge individual CSV files into a single file, clean data and push to Snowflake to create the data warehouse needed for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Python scripts from the ingest folder to scrape initial superbowl data (might need to change path, depending on how it's extracted on individual machines)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# ✅ Define the path to your scripts\n",
    "ingest_path = \"C:\\Work_Git\\DataEngineer\\CAPSTONE\"\n",
    "\n",
    "# ✅ Define the list of scripts to run in order\n",
    "scripts_to_run = [\n",
    "    \"merge_csv_datafiles.py\"\n",
    "    \"clean_data.py\",\n",
    "    \"upload_data_to_snowflake.py\",\n",
    "]\n",
    "\n",
    "# ✅ Execute each script\n",
    "for script in scripts_to_run:\n",
    "    script_path = os.path.join(ingest_path, script)\n",
    "    \n",
    "    if not os.path.isfile(script_path):\n",
    "        print(f\"❌ Script not found: {script}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"▶️ Running: {script}\")\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", script_path], check=True, capture_output=True, text=True)\n",
    "        print(f\"✅ Success: {script}\\n{result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Failed: {script}\\n{e.stderr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc3e5c",
   "metadata": {},
   "source": [
    "Build streamlit app from the superbowl data, first locally, then copy over to snowflake streamlit and run there, link on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Python scripts from the ingest folder to scrape initial superbowl data (might need to change path, depending on how it's extracted on individual machines)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# ✅ Define the path to your scripts\n",
    "ingest_path = \"C:\\Work_Git\\DataEngineer\\CAPSTONE\\streamlit_app\"\n",
    "\n",
    "# ✅ Define the list of scripts to run in order\n",
    "scripts_to_run = [\n",
    "    \"dashboard.py\"\n",
    "]\n",
    "\n",
    "# ✅ Execute each script\n",
    "for script in scripts_to_run:\n",
    "    script_path = os.path.join(ingest_path, script)\n",
    "    \n",
    "    if not os.path.isfile(script_path):\n",
    "        print(f\"❌ Script not found: {script}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"▶️ Running: {script}\")\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", script_path], check=True, capture_output=True, text=True)\n",
    "        print(f\"✅ Success: {script}\\n{result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Failed: {script}\\n{e.stderr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac1135",
   "metadata": {},
   "source": [
    "Run daily Pipeline to update current stock values (possible future implementation), but there is no real point to it, as this is only relevant once or twice a year, when Superbowl happens and a couple months after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db379304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run Python scripts from the ingest folder to scrape initial superbowl data (might need to change path, depending on how it's extracted on individual machines)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# ✅ Define the path to your scripts\n",
    "ingest_path = \"C:\\Work_Git\\DataEngineer\\CAPSTONE\"\n",
    "\n",
    "# ✅ Define the list of scripts to run in order\n",
    "scripts_to_run = [\n",
    "    \"daily_stock_pipeline.py\"\n",
    "]\n",
    "\n",
    "# ✅ Execute each script\n",
    "for script in scripts_to_run:\n",
    "    script_path = os.path.join(ingest_path, script)\n",
    "    \n",
    "    if not os.path.isfile(script_path):\n",
    "        print(f\"❌ Script not found: {script}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"▶️ Running: {script}\")\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", script_path], check=True, capture_output=True, text=True)\n",
    "        print(f\"✅ Success: {script}\\n{result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Failed: {script}\\n{e.stderr}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
